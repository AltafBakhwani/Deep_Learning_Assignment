{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 - Gradient Descent & Types of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm used to minimize functions, often used in machine learning and statistics to optimize model parameters. Here's an overview of Gradient Descent and its different types:\n",
    "\n",
    "Gradient Descent Overview\n",
    "Gradient Descent is an iterative method for finding the minimum of a function. The key idea is to update the parameters in the direction that reduces the function's value the most rapidly, which is opposite to the gradient direction.\n",
    "\n",
    "Basic Steps:\n",
    "Initialize the parameters randomly or using some heuristic.\n",
    "Compute the Gradient of the function with respect to the parameters.\n",
    "Update the Parameters by subtracting the gradient scaled by a learning rate.\n",
    "Repeat the process until convergence (i.e., the changes become negligibly small).\n",
    "Types of Gradient Descent\n",
    "Batch Gradient Descent\n",
    "\n",
    "Description: Uses the entire training dataset to compute the gradient of the cost function.\n",
    "Pros: Stable and deterministic because it always uses the same dataset.\n",
    "Cons: Can be very slow for large datasets, as it requires computing the gradient over the whole dataset before each update.\n",
    "Usage: Suitable for smaller datasets where the cost of computing gradients over the entire dataset is manageable.\n",
    "Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Description: Uses a single training example to compute the gradient and update the parameters.\n",
    "Pros: Much faster for large datasets since updates are made more frequently.\n",
    "Cons: The updates can be noisy and the path towards convergence can be erratic, making it harder to find the exact minimum.\n",
    "Usage: Effective for large datasets and online learning, where data is continuously fed into the model.\n",
    "Mini-Batch Gradient Descent\n",
    "\n",
    "Description: A compromise between Batch and Stochastic Gradient Descent. Uses a small, randomly chosen subset (mini-batch) of the training dataset to compute the gradient and update the parameters.\n",
    "Pros: Balances the efficiency of SGD and the stability of Batch Gradient Descent. Can make use of vectorized operations for faster computation.\n",
    "Cons: Requires tuning of mini-batch size, which can affect performance.\n",
    "Usage: Widely used in practice due to its efficiency and effectiveness, especially for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 - Validation set & Validation Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, a validation set and validation loss are crucial concepts for evaluating and tuning models during training. Here’s a detailed look at both:\n",
    "\n",
    "Validation Set\n",
    "What is a Validation Set?\n",
    "Definition: A validation set is a subset of the training data that is used to evaluate the performance of a machine learning model during the training process. It is separate from both the training set (used to train the model) and the test set (used to assess final model performance).\n",
    "Purpose: The validation set helps in tuning model hyperparameters, selecting the best model architecture, and preventing overfitting by providing an unbiased evaluation of the model during training.\n",
    "\n",
    "Validation Loss\n",
    "What is Validation Loss?\n",
    "Definition: Validation loss is the value of the loss function (error measure) computed on the validation set. It indicates how well the model is performing on unseen data that it has not been trained on.\n",
    "Purpose: Validation loss provides an indication of how well the model is generalizing to new, unseen examples. It helps in assessing the model’s performance and in making decisions about adjustments needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 - Create a MLP model step by step as we discussed in class and load tips data from Seaborn Library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load tips dataset\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(tips.head())\n",
    "\n",
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "tips = pd.get_dummies(tips, columns=['sex', 'smoker', 'day', 'time'], drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = tips.drop('tip', axis=1)\n",
    "y = tips['tip']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer and first hidden layer\n",
    "model.add(Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "# Add additional hidden layers\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(units=1, activation='linear'))  # Assuming regression task (predicting 'tip')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')  # Using mean squared error for regression\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nPredictions for the first 5 test samples:\")\n",
    "print(y_pred[:5])\n",
    "\n",
    "# Calculate and print test loss\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss}\")\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
